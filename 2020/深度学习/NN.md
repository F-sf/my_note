# 神经网络

## 激活函数

### softmax

一般用于输出层的激活函数，将所有Zi映射到[0,1]并使和为零，用以表示概率。
$$
S_i=\frac{e^i}{\sum_{j=1}^ne^j}\text{(n为元素总数)}
$$


## 损失函数Loss

### 交叉熵CrossEntropy

- 信息量I(x)：

  设事件发生概率为P(x)，则该事件发生概率越小，若其发生时包含的信息量则越大。
  $$
  I(x)=-log(P(x))
  $$

- 信息熵H(X)：

  表示信息量的期望（事件的混乱程度？）
  $$
  H(X)=-\sum_{i=1}^nP(x_i)log(P(x_i))
  $$

- 相对熵（KL散度）

  当对于同一个随机变量有两个单独概率分布P(X)和Q(x)时，用于描述二者的差异。
  $$
  D_{KL}(P||Q)=\sum_{i=1}^nP(x_i)log(\frac{P(x_i)}{Q(x_i)})
  $$
  展开可得
  $$
  D_{KL}(P||Q)=\sum_{i=1}^nP(x_i)log(P(x_i))-\sum_{i=1}^nP(x_i)log(Q(x_i))
  $$

  $$
  =-H(P(X))+[-\sum_{i=1}^nP(x_i)log(Q(x_i))]
  $$

  []中的后一部分即被称为交叉熵

  上式即为差异程度 = P和Q的交叉熵 - P的信息熵

  在神经网络中，P(X)为样本的真实分布，Q(X)为预测所得分布。故上式中的H(P(X))为一常数(单分类问题中甚至为0)，故可直接用交叉熵作为Loss

- 交叉熵作为LossFunc时一般配合softmax激活函数使用